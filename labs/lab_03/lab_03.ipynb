{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 03 - Regression\n",
    "## Tasks\n",
    "- Explore linear regression and overfitting\n",
    "- Experiment with stocastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/uspas/optimization_and_ml --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#matplotlib graphs will be included in your notebook, next to the code:\n",
    "%matplotlib inline\n",
    "\n",
    "#import toy accelerator package\n",
    "from uspas_ml.accelerator_toy_models import high_dimensional_data_generator\n",
    "\n",
    "#import sklearn modules\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#import pytorch\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression\n",
    "We start with simple least squares linear regression. Our model is defined as follows:\n",
    "\n",
    "$\n",
    "f(\\mathbf{x}) = \\mathbf{x}^T\\mathbf{w} + \\mathbf{b}\n",
    "$\n",
    "\n",
    "Our objective is to determine the weights $\\mathbf{w}$ and the biases $\\mathbf{b}$ for a set of input variables (also referred to as \"features\") to best model the observed values $y = f(\\mathbf{x}) + \\epsilon$ where $\\epsilon$ represents noise (usually Gaussian noise).\n",
    "\n",
    "We start by constructing a training set and a test set. We will train the model on the training set and then evaluate the model's abilitiy to correctly predict values from the test set. This (hopefully) allows us to evaluate if the model generalizes well to data outside the test set.\n",
    "\n",
    "One problem we face is overfitting (see https://en.wikipedia.org/wiki/Overfitting for more details). Here we will try to demonstrate methods for avoiding this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset - last column is f\n",
    "from uspas_ml.accelerator_toy_models import data_dir\n",
    "data = np.load(os.path.join(data_dir, 'complex_dataset.npy'))\n",
    "true_weights = np.load(os.path.join(data_dir, 'complex_dataset_weights.npy'))\n",
    "\n",
    "print(data.shape)\n",
    "x = data[:,:-1]\n",
    "f = data[:,-1]\n",
    "\n",
    "#split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, f, test_size=0.3, random_state=3)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit data with a linear model\n",
    "reg = linear_model.LinearRegression()\n",
    "reg.fit(X_train,y_train)\n",
    "reg.coef_\n",
    "\n",
    "#get training and test scores - least squared distance\n",
    "train_score = reg.score(X_train, y_train)\n",
    "test_score = reg.score(X_test, y_test)\n",
    "print(train_score)\n",
    "print(test_score)\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "ax.hist(reg.coef_, range = [-6,6]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "**Task:** \n",
    "    Refit the data using Ridge and Lasso regression with alpha = 0.01, 100 and plot histogram of weights Hint: use scikit-learn. Also plot a histogram of `true_weights` to compare. Also calculate the training and test scores for each. Which method is best for this data set?\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lasso regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Using SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector of 100 random numbers between zero and one; multiply all by 2.\n",
    "X = 2*np.random.rand(100,1)\n",
    "\n",
    "# Y(x) + dY, for random values of Y\n",
    "Y = 4 + 3*X + np.random.randn(100,1)\n",
    "\n",
    "plt.scatter(X, Y, label='gaussian noise abt. y=4x+3')\n",
    "print(data.shape)\n",
    "x = torch.from_numpy(X).float()\n",
    "f = torch.from_numpy(Y).float()\n",
    "\n",
    "\n",
    "#add data to dataset objects, which allows us to easily split data into train/test sets and shuffle the data as needed\n",
    "train_fraction = 0.7\n",
    "n_train = int(train_fraction*x.shape[0])\n",
    "n_test = x.shape[0] - n_train\n",
    "train_data, test_data = torch.utils.data.random_split(torch.utils.data.TensorDataset(x, f), [n_train, n_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a model to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a model class that we will train - represents a linear combination\n",
    "class LinearModel(torch.nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.linear = torch.nn.Linear(n_features, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# list trainable parameters for linear model if we have 10 inputs\n",
    "lmodel = LinearModel(2)\n",
    "for name, item in lmodel.named_parameters():\n",
    "    print(f'{name}:{item}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifications to Linear Regression\n",
    "The analytical solution for the weights using least squares involves matrix inversion, which makes it practically impossible to solve this problem analytically. The least squares loss function is defined as\n",
    "\n",
    "$\n",
    "    L(\\mathbf{w}) = ||X\\mathbf{w} - Y||^2\n",
    "$\n",
    "\n",
    "where $X$, $Y$ are matricies with our input and output data respecitvely. To minimize the loss we compute the gradient\n",
    "\n",
    "$\n",
    "    \\frac{dL}{d\\mathbf{w}} = -2X^TY + 2X^TX\\mathbf{w} \n",
    "$\n",
    "\n",
    "and set it to zero to find the minimum value for the weights, we get\n",
    "\n",
    "$\n",
    "    \\mathbf{w} = (X^TX)^{-1}X^T\\mathbf{y}\n",
    "$\n",
    "\n",
    "If $X$, $Y$ are large (maybe 10k x 10k) then the matrix inversion here becomes computationally impossible. \n",
    "\n",
    "Instead, we will optimize the weights numerically. However,computing the gradient is also expensive in this case.  Instead we will approximate the gradient using smaller \"batches\" of training points.\n",
    "\n",
    "We divide the entire dataset into batches and use the gradient with respect to these batches as a stand-in for the real gradient. Depending on the batch size we call this method as follows:\n",
    "- \"Batch gradient descent / Gradient descent\" -> the batch size is equal to the dataset size\n",
    "- \"Mini-batch gradient descent\" -> the batch size is > 1 but less than the dataset size\n",
    "- \"Stochastic gradient descent\" -> the batch size is 1\n",
    "\n",
    "Choosing the correct batch size has a significant effect on optimization speed depedning on the problem at hand. Reducing the batch size can make optimization faster to compute, but takes a noiser path. \n",
    "\n",
    "In this case the term \"epoch\" describes the number of times the optimizer will use the entire dataset during optimization (generally greater than one)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by trying batch gradient descent using `torch.optim.SGD`\n",
    "#### NOTE: even though the function name is SGD, it depends on how much data you pass it. In this case since we pass it the entire dataset it is effectively batch gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "gd_model = LinearModel(x.shape[1])\n",
    "\n",
    "#use Mean Squared Error loss function\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optim = torch.optim.SGD(gd_model.parameters(), lr = 0.01)\n",
    "\n",
    "batch_size = len(train_data)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "#track loss during epochs\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "for i in range(epochs):\n",
    "\n",
    "    #iterate over the batches (in this case we only have one batch)\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # zero gradients\n",
    "        optim.zero_grad()\n",
    "        \n",
    "        # calculate model prediction -> calculate loss\n",
    "        fval = gd_model(data)\n",
    "        loss = loss_fn(fval, target.reshape(-1,1))\n",
    "        \n",
    "        # calculate derivatives of loss based on model parameters\n",
    "        loss.backward()\n",
    "    \n",
    "        # step the optimizer based on the gradients\n",
    "        optim.step()\n",
    "        \n",
    "        # keep track of loss function\n",
    "        train_loss += [loss.detach().numpy()]\n",
    "        if i % 5 == 0:\n",
    "            print(f'epoch: {i}, loss: {loss}')   \n",
    "            \n",
    "        #calculate test loss\n",
    "        test_loss += [loss_fn(gd_model(test_data[:][0]),test_data[:][1].unsqueeze(1)).detach().numpy()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "**Task:** \n",
    "    Refit the data using stochastic gradient descent (batch size of 1) and mini-batch gradient descent with 10 batches. You will have to change the number of epochs so that it takes a reasonable amount of time (50 for mini-batch and 10 for SGD). Compare how changing batch size effects optimization by plotting test_loss as a function of epoch for each batch size. Compare how mini-batch training changes with varying learning rates (0.001, 0.01, 0.1).\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "**Homework:**\n",
    "    Provide an explaniation for why the MSE performance of gradient based methods varies so dramatically based on batch size.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "**Homework:**\n",
    "    How could we extend this method towards modeling more complex functions? Use a pytorch model to perform polynomial\n",
    "    regression of the dataset generated below. Your answer should closely match the analytical function used to generate the data.\n",
    "    Hint: You can access the weight elements of nn.Linear layer using `.weight`, see above for use.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(0,1, 100).unsqueeze(dim=1)\n",
    "y = 0.75 + 0.1 * x - 0.5 * x ** 2 + 0.5 * x ** 3 + torch.randn(x.shape)*0.005\n",
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
